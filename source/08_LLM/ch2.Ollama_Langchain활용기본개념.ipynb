{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25b46c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "div.container{width:99% !important;}\n",
       "div.cell.code_cell.rendered{width:100%;}\n",
       "div.input_prompt{padding:0px;}\n",
       "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
       "div.text_cell_render.rendered_html{font-size:20pt;}\n",
       "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
       "div.output {font-size:24pt; font-weight:bold;}\n",
       "div.input {font-family:Consolas; font-size:24pt;}\n",
       "div.prompt {min-width:70px;}\n",
       "div#toc-wrapper{padding-top:120px;}\n",
       "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
       "table.dataframe{font-size:24px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "div.container{width:99% !important;}\n",
    "div.cell.code_cell.rendered{width:100%;}\n",
    "div.input_prompt{padding:0px;}\n",
    "div.CodeMirror {font-family:Consolas; font-size:24pt;}\n",
    "div.text_cell_render.rendered_html{font-size:20pt;}\n",
    "div.text_cell_render ul li, div.text_cell_render ol li p, code{font-size:22pt; line-height:30px;}\n",
    "div.output {font-size:24pt; font-weight:bold;}\n",
    "div.input {font-family:Consolas; font-size:24pt;}\n",
    "div.prompt {min-width:70px;}\n",
    "div#toc-wrapper{padding-top:120px;}\n",
    "div.text_cell_render ul li{font-size:24pt;padding:5px;}\n",
    "table.dataframe{font-size:24px;}\n",
    "</style>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1992d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38e405df",
   "metadata": {},
   "source": [
    "## ch2.Ollama_LLM활용의 기본 개념(LangChain)\n",
    "1. LLM을 활용하여 답변 생성하기\n",
    "1) Ollama를 이용한 로컬 LLM 이용\n",
    "- 성능은 GPT, CLaude 같은 모델보다 떨어지나, 개념설명을 위해 open source 모델 사용\n",
    "\n",
    "- ollama.com 다운로드 -> 설치 -> 모델 pull\n",
    "- cmd창이나 powershell 창에 ollama pull deepseek-r1:1.5b https://docs.langchain.com/oss/python/integrations/chat/ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2e45b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 환경변수 가져오기\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "# os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61f90a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Seoul', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 587, 'prompt_tokens': 22, 'total_tokens': 609, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 576, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CkkbPT5nIezovbDzr4sd2th66Bkgh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b0197-79bd-73c0-9c43-b64e690aea73-0', usage_metadata={'input_tokens': 22, 'output_tokens': 587, 'total_tokens': 609, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 576}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
    "                #api_key=os.gentenv(\"OPENAI_API_KEY\")\n",
    "                 \n",
    "result = llm.invoke(\"What is the capitaal of Korea? Return the name of the city only.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe11eb",
   "metadata": {},
   "source": [
    "# 모델 pull\n",
    "- cmd창이나 powershell창(window키+R에서 powershell)에서 ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db3957b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ab31dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국의 수도는 서울입니다.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"한국 수도가 어디예요?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "334afea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'일본의 수도는 도야กु (도يgasu)입니다.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"일본 수도가 어디예요?\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54e4c2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Korea is Seoul. However, the official administrative center of South Korea is Pyongyang, which is located in North Korea (also known as North Korea or DPRK).', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:18:23.8340253Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3226202000, 'load_duration': 1668538800, 'prompt_eval_count': 32, 'prompt_eval_duration': 309337800, 'eval_count': 37, 'eval_duration': 1205709800, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01c2-c63e-7413-9a67-12c28c451a5c-0', usage_metadata={'input_tokens': 32, 'output_tokens': 37, 'total_tokens': 69})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "result = llm.invoke(\"What is the capital of Korea?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b341d5",
   "metadata": {},
   "source": [
    "## 2) openai 활용\n",
    "- pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "821e8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-5-nano\")\n",
    "result=llm.invoke(\"What is the capital of Korea? Return the name of the city only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3261ff93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Seoul', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 523, 'prompt_tokens': 21, 'total_tokens': 544, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 512, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CklKuY2Mizf3N5AzwHvgCSu49lLcv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b01c2-8517-7cc3-a153-ede070ad2de7-0', usage_metadata={'input_tokens': 21, 'output_tokens': 523, 'total_tokens': 544, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 512}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ee9634",
   "metadata": {},
   "source": [
    "# 2. 렝체인 스타일로 프롬프트 작성\n",
    "- 프롬프트: llm 호출시 쓰는 질문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "482be635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "# llm.invoke(0)\n",
    "# PromptValue, str, BaseMessages리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eddf19",
   "metadata": {},
   "source": [
    "## 1) 기본 프롬프트 템플릿 사용\n",
    "- PromptTemplate을 사용하여 변수가 포함된 템플릿을 작성하면 PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24653d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea?'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Korea is Seoul.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:18:48.825316Z', 'done': True, 'done_reason': 'stop', 'total_duration': 520209300, 'load_duration': 124417200, 'prompt_eval_count': 32, 'prompt_eval_duration': 138321200, 'eval_count': 8, 'eval_duration': 249380500, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01c3-326f-79b0-8703-73b37c297765-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"What is the capital of {country}?\", # {}안의 값을 새로운 값으로 대입 가능\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# country = input('어느 나라의 수도를 알고 싶으신가요?')\n",
    "prompt = prompt_template.invoke({\"country\":\"Korea\"})\n",
    "print(prompt)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07505a93",
   "metadata": {},
   "source": [
    "# 2) 메세지 기반 프롬프트 작성\n",
    "- BaseMessage 리스트\n",
    "- BaseMessage 상속받은 클래스 : AIMessage, HumanMessage, SystemMessage, ToolMessage\n",
    "- vscode에서 ctrl+shift+p : python:select interpreter입력 -> python환경선택\n",
    "- vscode에서 커널 선택\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "541a12ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of South Korea is Seoul, and the capital of North Korea is Pyongyang. However, it\\'s worth noting that the two countries are separated by a border and do not have direct control over each other\\'s governments or territories. The international community often uses the term \"North Korean\" to refer specifically to the northern part of the country.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:18:54.4948823Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3297065200, 'load_duration': 135138500, 'prompt_eval_count': 86, 'prompt_eval_duration': 582267100, 'eval_count': 69, 'eval_duration': 2510101300, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01c3-3dbc-74e0-93a6-52bbce0b3ffb-0', usage_metadata={'input_tokens': 86, 'output_tokens': 69, 'total_tokens': 155})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), # 페르소나 부여\n",
    "    HumanMessage(content=\"What is the capital of Italy?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of France?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of Korea?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49b4121a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"If you don't mention a specific country, I can try to provide the capital for various countries. Keep in mind that I may not have the most up-to-date information, but I'll do my best to provide an accurate answer.\\n\\nWhich country would you like to know the capital for?\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T06:21:19.8848652Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2756609800, 'load_duration': 199782300, 'prompt_eval_count': 87, 'prompt_eval_duration': 96939500, 'eval_count': 59, 'eval_duration': 2393593700, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01c5-77c6-7ba0-9d5c-0497183b68b5-0', usage_metadata={'input_tokens': 87, 'output_tokens': 59, 'total_tokens': 146})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), # 페르소나 부여\n",
    "    HumanMessage(content=\"What is the capital of Italy?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of France?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of {country}?\")\n",
    "]\n",
    "llm.invoke(message_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f9bbec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You are a helpful assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of {country}?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "message_list = [\n",
    "    SystemMessage(content=\"You are a helpful assistant!\"), # 페르소나 부여\n",
    "    HumanMessage(content=\"What is the capital of Italy?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of Italy is Rome.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of France?\"), # 모범질문\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),    # 모범답안\n",
    "    HumanMessage(content=\"What is the capital of {country}?\")\n",
    "]\n",
    "print(message_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f51911",
   "metadata": {},
   "source": [
    "## 3) ChatPromptTemplate 사용\n",
    "- BaseMessage리스트 -> 튜플리스트 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283f239c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9716913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요korea\n",
      "프롬프트 :  messages=[SystemMessage(content='You are a helpfull assistant!', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of Italy?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of Italy is Rome.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of France?', additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the capital of korea?', additional_kwargs={}, response_metadata={})] <class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "source": [
    "# 위의 BaseMessage 리스트를 수정\n",
    "# PromptTemplate : 프롬프트에 변수포함, \n",
    "# ChatPromptTemplate : SystemPrompt설정(페르소나), few shot설정, 변수포함\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "chatPrompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpfull assistant!\"),\n",
    "    ('human', \"What is the capital of Italy?\"), # 모범질문\n",
    "    (\"ai\", \"The capital of Italy is Rome.\"),    # 모범답안\n",
    "    ('human', \"What is the capital of France?\"), # 모범질문\n",
    "    (\"ai\", \"The capital of France is Paris.\"),    # 모범답안\n",
    "    (\"human\", \"What is the capital of {country}?\")\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요\")\n",
    "prompt = chatPrompt_template.invoke({\"country\": country})\n",
    "print(\"프롬프트 : \", prompt, type(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c18a54cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요?korea\n",
      "content='대화의 방법에 대한 질문을 받으시고 싶습니다. 대화에 대해 더 많은 information이 필요하면, 질문에 대한 추가적인 정보를 주세요.' additional_kwargs={} response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T07:02:37.1740213Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1655838300, 'load_duration': 137148300, 'prompt_eval_count': 44, 'prompt_eval_duration': 252844800, 'eval_count': 37, 'eval_duration': 1228096300, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'} id='lc_run--019b01eb-48fc-72f2-8b06-1ff3526791af-0' usage_metadata={'input_tokens': 44, 'output_tokens': 37, 'total_tokens': 81}\n"
     ]
    }
   ],
   "source": [
    "chatPrompt_Template=ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 대한민국 정보 전문 도우미입니다\"),\n",
    "    (\"human\", \"{country}의 수도가 어디예요?\")\n",
    "])\n",
    "country= input(\"어느 나라 수도가 궁금하세요?\")\n",
    "prompt=chatPrompt_Template.invoke({\"country\":country})\n",
    "# print(prompt)\n",
    "result=llm.invoke(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "449001d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어느 나라 수도가 궁금하세요korea\n",
      "한국의 수도는 Seoul이며, 이 곳은 한국에서 가장大的 도시로, 국가의 정치 및 문화 지배를 받고 있습니다. Seoul은 한국 역사와 문화에 큰影響을 주고 있으며, 다양한 종류의 건물과 시설이 있습니다.\n"
     ]
    }
   ],
   "source": [
    "chatPromptTemplate = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"당신은 대한민국 정보 전문 도우미입니다\"),\n",
    "    (\"human\", \"{country}의 수도가 어디예요!\")\n",
    "])\n",
    "country = input(\"어느 나라 수도가 궁금하세요\")\n",
    "prompt = chatPromptTemplate.invoke({\"country\":country})\n",
    "# print(prompt)\n",
    "result = llm.invoke(prompt)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad8fb78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='한국의 수도는 Seoul이며, 이 곳은 한국에서 가장大的 도시로, 국가의 정치 및 문화 지배를 받고 있습니다. Seoul은 한국 역사와 문화에 큰影響을 주고 있으며, 다양한 종류의 건물과 시설이 있습니다.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-12-09T07:03:37.4412188Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2200420000, 'load_duration': 139658400, 'prompt_eval_count': 44, 'prompt_eval_duration': 101094800, 'eval_count': 56, 'eval_duration': 1907137300, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b01ec-3247-7a11-9720-29548e9cf704-0', usage_metadata={'input_tokens': 44, 'output_tokens': 56, 'total_tokens': 100})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a12c0e",
   "metadata": {},
   "source": [
    "# 3. 답변 형식 컨트롤하기\n",
    "- llm.invoke()의 결과는 AIMessage()-> string이나 json, 객체: OutputParser이용\n",
    "## 1) 문자열 출력 파서 이용\n",
    "- StrOutputParser를 사용하거나 LLM출력(AIMessage)을 단순 문자열로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8a84149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='What is the capital of Korea. Retrun the name of the city only.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시하상이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a8f019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시하상이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "# print(prompt)\n",
    "ai_message=llm.invoke(prompt)\n",
    "# output_parse=StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "badc1d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# 명시적인 지시하상이 포함된 프롬프트\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "# 프롬프트 템플릿에 값 주입\n",
    "prompt = prompt_template.invoke({\"country\": \"Korea\"})\n",
    "# print(prompt)\n",
    "ai_message = llm.invoke(prompt)\n",
    "# print(ai_message)\n",
    "# 문자열 출력 파서를 이용하여 llm응답(AIMessage객체)을 단순 문자열로 변환\n",
    "output_parser = StrOutputParser()\n",
    "result = output_parser.invoke(ai_message)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "861e012f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "967d3023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokyo'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 변수설정, system, few shot 지정\n",
    "chat_prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpfull assistant with experience in South Korea\"),\n",
    "    ('human', \"What is the capital of Italy?\"), # 모범질문\n",
    "    (\"ai\", \"Rome\"),    # 모범답안\n",
    "    ('human', \"What is the capital of France?\"), # 모범질문\n",
    "    (\"ai\", \"Paris\"),    # 모범답안\n",
    "    (\"human\", \"What is the capital of {country}? Retrun the name of the city only\")\n",
    "])\n",
    "output_parser=StrOutputParser()\n",
    "output_parser.invoke(llm.invoke(chat_prompt_template.invoke({\"country\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd2bd8",
   "metadata": {},
   "source": [
    "# 2) Json 출력 파서 이용\n",
    "- {'name':'홍길동','age':22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40e3d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "{\n",
      "    \"capital\": \"Seoul\",\n",
      "    \"population\": 51000000,\n",
      "    \"language\": \"Korean\",\n",
      "    \"currency\": \"Won\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "country_detail_prompt=PromptTemplate(\n",
    "    template=\"\"\"Give following information about {country}.\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    Return in JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]    \n",
    ")\n",
    "prompt=country_detail_prompt.invoke({\"country\": \"Korea\"})\n",
    "ai_message=llm.invoke(prompt)\n",
    "print(ai_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3c2f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capital': 'Seoul', 'population': 51.8, 'language': 'Korean', 'currency': 'KRW'} <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "country_detail_prompt=PromptTemplate(\n",
    "    template=\"\"\"Give following information about {country}.\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    Return in JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]    \n",
    ")\n",
    "prompt=country_detail_prompt.invoke({\"country\": \"Korea\"})\n",
    "ai_message=llm.invoke(prompt)\n",
    "#print(ai_message.content)\n",
    "output_parser=JsonOutputParser()\n",
    "json_result=output_parser.invoke(ai_message)\n",
    "print(json_result,type(json_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "193a392b",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Invalid json output: ```\n{\n    \"capital\": \"Tokyo\",\n    \"population\": 128,661,000,\n    \"language\": \"Japanese\",\n    \"currency\": \"Japanese yen\"\n}\n```\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:84\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparse_json_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:156\u001b[0m, in \u001b[0;36mparse_json_markdown\u001b[1;34m(json_string, parser)\u001b[0m\n\u001b[0;32m    155\u001b[0m     json_str \u001b[38;5;241m=\u001b[39m json_string \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:172\u001b[0m, in \u001b[0;36m_parse_json\u001b[1;34m(json_str, parser)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Parse the JSON string into a Python dictionary\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\utils\\json.py:129\u001b[0m, in \u001b[0;36mparse_partial_json\u001b[1;34m(s, strict)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# If we got here, we ran out of characters to remove\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# and still couldn't parse the string as JSON, so return the parse error\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# for the original string.\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\__init__.py:359\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    358\u001b[0m     kw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_constant\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_constant\n\u001b[1;32m--> 359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 3 column 23 (char 48)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_detail_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcountry\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJapen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:200\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[1;32m--> 200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    211\u001b[0m         config,\n\u001b[0;32m    212\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\runnables\\base.py:2058\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   2054\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m   2055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   2056\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   2057\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 2058\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   2059\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m                 func,\n\u001b[0;32m   2061\u001b[0m                 input_,\n\u001b[0;32m   2062\u001b[0m                 config,\n\u001b[0;32m   2063\u001b[0m                 run_manager,\n\u001b[0;32m   2064\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2065\u001b[0m             ),\n\u001b[0;32m   2066\u001b[0m         )\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   2068\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\runnables\\config.py:433\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    432\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\base.py:201\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[1;34m(inner_input)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    198\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m--> 201\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    205\u001b[0m             config,\n\u001b[0;32m    206\u001b[0m             run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m         )\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([Generation(text\u001b[38;5;241m=\u001b[39minner_input)]),\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    211\u001b[0m         config,\n\u001b[0;32m    212\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    213\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\llm\\lib\\site-packages\\langchain_core\\output_parsers\\json.py:87\u001b[0m, in \u001b[0;36mJsonOutputParser.parse_result\u001b[1;34m(self, result, partial)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     86\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid json output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg, llm_output\u001b[38;5;241m=\u001b[39mtext) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOutputParserException\u001b[0m: Invalid json output: ```\n{\n    \"capital\": \"Tokyo\",\n    \"population\": 128,661,000,\n    \"language\": \"Japanese\",\n    \"currency\": \"Japanese yen\"\n}\n```\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "output_parser.invoke(llm.invoke(country_detail_prompt.invoke({\"country\":\"Japen\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbdc001",
   "metadata": {},
   "source": [
    "## 3) 구조화된 출력 사용\n",
    "- Pydantic 모델을 사용하여 LLM출력을 구조화된 형식으로 받기(JsonParser보다 훨씬 안정적)\n",
    "- Pydantic : 데이터유효성검사, 설정관리를 간편하게 해주는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38a62e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.User object at 0x00000204678033A0>\n"
     ]
    }
   ],
   "source": [
    "class User:\n",
    "    def __init__(self, id, name, is_active=True):\n",
    "        self.id   = id\n",
    "        self.name = name\n",
    "        self.is_active = is_active\n",
    "user = User(\"1\", \"홍길동\", False)\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34c9b7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=1 name='홍길동' is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class User(BaseModel):\n",
    "    # gt=0:id>0 / ge=0:id>=0 / lt=0:id<0 / le=0:id<=0\n",
    "    id:int   = Field(gt=0,         description=\"id\")\n",
    "    name:str = Field(min_length=2, description=\"name\")\n",
    "    is_active:bool = Field(default=True, description=\"id활성화 여부\")\n",
    "user = User(id=\"1\", name=\"홍길동\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "841ebde3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountryDetail(capital='Seoul', population=51000000, language='Korean', currency='South Korean won')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_detail_prompt = PromptTemplate(\n",
    "    template = \"\"\"Give following information about {country}.\n",
    "    - Capital\n",
    "    - Population\n",
    "    - Language\n",
    "    - Currency\n",
    "    Return in JSON format and return the JSON dictionary only\"\"\",\n",
    "    input_variables=[\"country\"]\n",
    ")\n",
    "class CountryDetail(BaseModel): #description: 더 정확한 출력 유도\n",
    "    capital:str  = Field(description=\"the capital of the country\")\n",
    "    population:int = Field(description=\"the population of the country\")\n",
    "    language:str = Field(description=\"the language of the country\")\n",
    "    currency:str = Field(description=\"the currency of the country\")\n",
    "# 출력 형식 파서 + LLM\n",
    "structedllm = llm.with_structured_output(CountryDetail)\n",
    "# llm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "info = structedllm.invoke(country_detail_prompt.invoke({\"country\":\"Korea\"}))\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "731516d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CountryDetail"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1741bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Seoul', 51000000, 'Korean', 'South Korean won')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.capital, info.population, info.language, info.currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3765114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json 스타일로 : {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean\",\"currency\":\"South Korean won\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"info를 json 스타일로 :\", info.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "80589fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 dict로 : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print(\"info를 dict로 :\", info.model_dump())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c2f41eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 dict로 : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print(\"info를 dict로 :\", info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ceadf076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info를 json 스타일로 : {\"capital\":\"Seoul\",\"population\":51000000,\"language\":\"Korean\",\"currency\":\"South Korean won\"}\n",
      "info를 dict로 : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n",
      "info를 dict로 : {'capital': 'Seoul', 'population': 51000000, 'language': 'Korean', 'currency': 'South Korean won'}\n"
     ]
    }
   ],
   "source": [
    "print(\"info를 json 스타일로 :\", info.model_dump_json())\n",
    "print(\"info를 dict로 :\", info.model_dump())\n",
    "print(\"info를 dict로 :\", info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f9defb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(info.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd9038",
   "metadata": {},
   "source": [
    "# 4. LCEL을 활용한 랭체인 생성하기\n",
    "## 1) 문자열 출력 파서 사용\n",
    "- invoke\n",
    "- StrOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84ff1bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\",\n",
    "                temperature=0) # 일관된 답변(보수적인 답변)\n",
    "prompt_template = PromptTemplate(\n",
    "    template = \"What is the capital of {country}. Retrun the name of the city only.\",\n",
    "    input_variables = [\"country\"]\n",
    ")\n",
    "output_parser = StrOutputParser() # AIMessage()를 Str변환\n",
    "output_parser.invoke(llm.invoke(prompt_template.invoke({\"country\":\"Korea\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc264c0",
   "metadata": {},
   "source": [
    "## 2) LCEL을 사용한 간단한 체인 구성\n",
    "- 파이크연산자(|) 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "da2e4025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seoul'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트 템플릿 -> llm -> 출력파서를 연결하는 체인 생성\n",
    "capital_chain = prompt_template | llm | output_parser\n",
    "# 생성된 체인 invoke\n",
    "capital_chain.invoke({\"country\":\"Korea\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94c1911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d14f5697",
   "metadata": {},
   "source": [
    "## 3) 복합체인 구성\n",
    "- 여러 단계의 추론이 필요한 경우(체인 연결)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d64c4638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명 -> 나라이름\n",
    "country_prompt = PromptTemplate(\n",
    "    template=\"\"\"Guess the name of the country based on the following informat:\n",
    "    {information}\n",
    "    Return the name of the country only\"\"\",\n",
    "    input_variables=[\"information\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                            \"This country is very famous for its wine\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1017d213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Italy'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라명 추출 체인 생성\n",
    "country_chain = country_prompt | llm | output_parser\n",
    "country_chain.invoke({\"information\":\"This country is very famous for its wine\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ddb6b2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 복합체인 : 나라설명 -> 나라명(country_chain)\n",
    "#                      나라명 -> 수도(capital_chain)\n",
    "final_chain = country_chain | capital_chain\n",
    "final_chain.invoke({\"information\":\"This country is very famous for its wine\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da0e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복합체인 : information -> country_chain -> (나라명을 country) -> capital_chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "final_chain = {\"information\":RunnablePassthrough()} | \\\n",
    "                {\"country\":country_chain} | capital_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "668edd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rome'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke(\"This country is very famous for its wine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82aed3",
   "metadata": {},
   "source": [
    "- 한글 지원이 안 되는 모델은 렝체인 연결이 잘 안 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41cb9da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 나라는 와인으로 유명해\\n\\n1. 프랑스 - burgundy, merlot, cabernet sauvignon\\n2. 이탈리아 - chianti, pinot noir, vermentino\\n3. 스태프lein만 - merlot, cabernet sauvignon, syrah\\n4. 이스라엘 - shiraz, carmenère, pinot noir\\n5. 독일 - riesling, pinot noir, gruner veltliner'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 나라 설명 -> 나라이름\n",
    "country_prompt = PromptTemplate(\n",
    "    template=\"\"\"다음의 {information} 설명을 보고 나라이름을 맞춰봐:\n",
    "    {information}\n",
    "    나라 이름만 한국어로 reutrn 해 줘\"\"\",\n",
    "    input_variables=[\"information\"]\n",
    ")\n",
    "output_parser.invoke(llm.invoke(country_prompt.invoke({\"information\":\n",
    "                            \"이 나라는 와인으로 유명해\"})))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c66bd",
   "metadata": {},
   "source": [
    "# 5. 생성형 AI평가: 나라명을 입력하면 그 나라의 제일 유명한 음식의 레시피를 출력하는 복합체인 구현\n",
    "- 위 : 나라설명 -> 나라이음 -> 수도(체인 두개 연결)\n",
    "- 나라이름- 그 나라에서 제일 유명한 음식-> 레시피"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa87e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복합체인 : 나라설명 -> 그 나라에서 제일 유명한 음식(food_chain)1\n",
    "#                      음식 -> 레시피(recipe_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be3f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061d5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e551e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4517d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b686f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccb18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24d9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687958fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm(ipykernel)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
